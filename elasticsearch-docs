sudo sysctl -w vm.max_map_count=262144

To Install Elastic Search:
0) sudo sysctl -w vm.max_map_count=262144 // Only if throws error to increase virtual memory for docker
1) docker network create elastic
2) docker run -d -e ES_JAVA_OPTS="-Xms512m -Xmx512m" -e ELASTIC_PASSWORD="elastic" -v datanode0:/usr/share/elasticsearch/data --name ngp-elastic-0 -p 9200:9200 -p 9300:9300 --net elastic  docker.elastic.co/elasticsearch/elasticsearch:8.4.1
3) docker cp ngp-elastic-0:/usr/share/elasticsearch/config/certs/http_ca.crt .
4) curl --cacert http_ca.crt -u "elastic:elastic" https://localhost:9200

To Generate Enrollment token for elastic node:
5) docker exec -it ngp-elastic-0 /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s node
6) docker run -e ENROLLMENT_TOKEN="<token>" --name ngp-elastic-1 -v datanode1:/usr/share/elasticsearch/data --net elastic -it docker.elastic.co/elasticsearch/elasticsearch:8.4.1

TO Generate new Enrollement token for kibana:
7) docker exec -it ngp-elastic-0 /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana

TO Reset Elastic Password:
8) docker exec -it ngp-elastic-0 /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic


To Install Kibana:
1) docker pull docker.elastic.co/kibana/kibana:8.4.1
2) docker run -d --name ngp-kibana --net elastic -p 5601:5601 docker.elastic.co/kibana/kibana:8.4.1


Elastic Search:

Node- Instance of elastic search, any number of nodes can run in a single machine
Cluster- group of nodes of same instances
Document- Each unit of data stored in elastic search is called document, json object.
Documents can be equialent to tables in sql, fields can be equivalent to column in tables
Indices- Collections of documents with similar characteristics


Summary-1:
1) Node store data that we add to Elastic Search
2) A Cluster is a collection of nodes
3) Data is stored as documents which are jsonObjects
4) Documents are grouped together with indices

Inspect The Elastic Search Cluster:
1) Indices with prefix dot(.) indicates its a system indices
2) TLS is required when communicating with elastic user Https instead of http
3) For Local Deployments Specify either --insecure or --cacert
4) Requests must be authenticated use elastic user and auto-generated passwords
5) Specify Content-Type header when sending request body

Sharding & Scalability:
1) Sharding is a way to split index into smaller pieces each being a shard
2) Sharding helps to grow index in size, through put in size 
3) Consider a elastic with node size of 50Gb, now if you have a single index with 70GB it would be fit in a single index, not if you shard your index into 2 or multiple shards then each shard may present in different node.
4) Sharding makes it easier to fit larger index onto nodes
5) Default index has only 1 shard, can be configured while creating a index
6) Sharding helps in distributed query
7) Sharding enables us to scale the data volume. Adding more nodes to the cluster helps too, but only to a certain extent (unless there are no very large indices).
8) An index is divided into one or more shards, where each shard stores a part of the index' data.
9) An index will contain one shard for Elasticsearch >= 7.0.0.


Replication:
1) Hardware can fail at anytime so to keep your safe you need to replicate your data so that it can be fetched from other node
2) ElasticSearch supports replication by default
3) Replication is configured at index level
4) Replication works by creating a replica of shards that a index can contain, refered to as replicas or replica shards
5) A shard that has been replicated is called primary shard
6) A primary shard and its replica shards are called replication group
7) Replica shards are a complete copy of primary shards, which can serve search requests just like primary shards
8) The number of replica shards can be configured at index creation
9) Replica shards are never stored in same node as primary shard
10) Replication helps only if you have more than 1 node else no point in replication
11) Even tho replication is configured at index creation elastic is only going to replicate only there are more than 1 node
12) Replica shards of a replication group can server different requests simultaniously this increases the no of requests can be handled at the same time
13) Elasic intelligently routes requests to the best shards based on number of parameters
14) Assume you have 1 node elastic cluster and you have created a new index, then you can see immediately index and cluser state is changed to yellow because the default 1 replica shard is unassigned as there is only 1 node elastic wouldnt assign replica shard to the same node. Yellow state is a warning that data loss can happen if the node is lost or crashed as the replica shard is not assigned.
15) Assume you have 1 node cluster and default kibana indices are having 0 replicas because kibana indices are configures with a settings called "auto_expand_replicas with value of 0-1" when a new node is available
16) Replication ensures that if a node goes down, data will be available on a different node (provided that the cluster consists of more than one node).
17) By default for a newly created index 2 shards are created (1 primary shard + 1 replica shard)


Snapshots:
1) Elasticsearch supports taking backups as snapshots to restore data
2) Snapshot can be used to restore to a given point in time
3) Snapshots can be taken at index level or for entire cluster
4) Use snapshots for daily backups and replication for high availability and performance

Node Roles:
1) Data in indices are stored in shards and shards are stored in nodes based on the roles of the node
2) you can find the node role in the nodes apis where node.role column tells (dim stands for data,ingest,master)
Mater Role:
1) A Node can be elected as master node by voting process
2) Configuration node.master=true|false can be given to multiple nodes 
3) A master node is responsible for creating/deleting indexes among others
4) A node with master role doesnt automatically becomes master node unless there are no other master nodes available

Data Role:
1) Enables a node to store data
2) Storing data includes performing search queries relates to the data
3) for smaller clusters this role is already enabled 
4) Used for having dedicated master nodes
5) Configuration: node.data=true|false

Ingest Role:
1) Enables a node to run ingest pipeline
2) Ingesting refers to adding a document to a index
3) A simplified version of LogStash functionality within elastic
4) Configuration node.ingest=true|false

Machine Learning Role:
1) node.ml identifies a node as a machine learning node, this lets node run machine learning jobs
2) Configuration: node.ml=true|false
3) Configuration: xpack.ml.enabled=true|false enables or disables the machine learning apis for the node
4) Useful for running machine learning jobs without effecting other tasks

Coordination Role:
1) Coordination refers to the distrubution of queries and aggregate results
2) Configuration need for this role -> node.master=false, node.data=false, node.ml=false, node.ingest=false, xpack.ml.enabled=false
3) this role is configured by disabling all other nodes
4) useful for coordination when you have large clusters

Voting-only Role:
1) Rarely used, you wont use it either
2) Configuration node.voting-only=true|false
3) A node with this role will participate in voting the new master node, but cannot be elected for master node

When to change node roles:
1) Typically used for large cluster
2) When optimizing cluster to scale the no of requests
3) Better understanding what hardware resources are used for
4) Only change the roles when you have clear idea what you are doing


Managing Documents:

1) Elastic documents are immutable
2) When we update document it retrives the document, fields values are changed, existing document is replaced with modified document


Routing:
1) How does elastic search know where to store documents ?
2) How does documents are found once they are index
3) Answer is routing
4) Routing is a process of resolving a shard for a document
5) when we store a document elastic search uses a simple formula to find the shared "shard_num=hash(_routing) % num_primary_shards"
6) This process is only when we try with id
7) we can change the default routing strategy
8) Default routing strategy distributes documents evenly among shards


Reading document:
1) A read request is received by a coordinating node
2) Routing is used to resovle the replication group
3) Adaptive Replica Selection(ARS) is used to send the query to the best suitable shard
4) ARS helps reduce query response time
5) ARS is a intelligent load balancer
6) The coordinating node collects the response and send to the client

Writing document:
1) A write request is received by coordinator node
2) Routing is use to resolve the replication group
3) write operations are sent to primary shard
4) primary terms and sequence numbers are used to recover from failures
5) global and local checkpoints helps speed up the recovery process
6) primary terms and sequence numbers are available within response
7) incase of primary shard failure one of the replicas shard is made as primary shard


Primary terms:
1) Are the way for elastic serach to distinguish between old and new primary shards
2) Essentially a counter for how many times primary shard has been changed

Sequence Numbers:
1) Appended to primary terms with the write operations
2) Essentially a counter that is incremented for each write operation
3) The primary shard increases the sequence number
4) Enabled elastic search to perform write operation

Recovering from a primary shard failure:
1) primary terms and sequence numbers are key when elastic search need to recover from a primary shard
2) Enables elastic to more efficently figure out which write operation to be performed
3) For large indices this process is really expensive
4) To speed up this process elastic search uses checkpoints

Global and local check points:
1) Global checkpoints are kept at replication group
2) Local checkpoints are kept at replica shard
3) Local checkpoint is the sequence number for the last write operation that is performed

Versioning the document:
1) It is not a revision history of documents, cannot retrive older document
2) elasticsearch stores the "_version" field in every document
3) it is integer field and is incremented everytime document is updated
4) value is retained for 60 secs after deleting the document
5) this is the default versioning and is refered to as Internal versioning
6) External versioning type is used to maintain version outside the elastic search. ex: when documents are stored in rdbms
7) you can tell how many times a document has been modified
8) versioning is handly used now, it was used for Optimistic Concurrency Control, now there is a better way to handle


Optimistic Concurrency Control:
1) Sending write requests to elasticserach concurrently may overwrite the changes made by other
2) Traditionally sending "_version" field in query parameter helps to prevent this
3) Today we use "_primary_term" and "_seq_no" fields
4) Elasticsearch will reject the operation if the primary tern and seq_no fields doesnt match with the latest document in elastic
5) This should be handled at application level


Update Multiple Documents based on query:
0) Called as "_update_by_query"
1) The Query creates a snapshot of the index for Optimistic Concurrency Control
2) Search Queries and bulk requests are sent to the replication group sequentially, elastic retried these queries up to 10times, if the query sill fails the query is abonded and changes are not rolled back.
3) Api Returns information about failures
4) If a document is modified after taking the snapshot the query is aborted, this is checked from primary_term and seq_no
5) To count version conflicts instead of aborting the query, the "conflicts" option in the query is set to "proceed".


Delete Multiple Documents based on query:
1) called as "_delete_by_query"
2) Rest all things are performed same as "_update_by_query"
3) creates snapshot, prepares batches, process them sequentially, aborted is snapshot is modified dure to conflicts


Bulk API:
1) we can perform index,create,update,delete on multiple documents with bulk api
2) Bulk API expects to be in NDJSON specification
example:
	action_metadata\n
	optional_source\n
	action_metadata\n
	optional_source\n
2) Create document will fail if the document already exist
3) Index adds document if not exist else replace
4) "Content-Type: application/x-ndjson" should be passed in headers
5) application/json is also accepted but not the right way
6) Elastic SDKs handles this for us but we need to specify for elastic clients 
7) Each line must end with new charecter(\n or \r\n)
8) A failed action will not affect other actions
9) Detailed information about each action is returned
10) More efficient than sending individual requests
11) Routing is used to resolve each documents shard
12) Bulk api supports Optimistic Concurrency Control, you can include "if_primary_term" and "if_seq_no" with in action metadata


Importing data with curl:
curl -XPOST -H "Content-Type: application/x-ndjson" -u "elastic:elastic" http://localhost:9200/products/_bulk --data-binary "@products-bulk.json"


#######################################################
################### ANALYSIS ##########################
#######################################################

1) Also called text analysis
2) Applicable to text fields/values
3) Text values are analyzed when indexing documents
4) The result in sored in data structure that is efficent for searching
5) "_source" object is not used when searching for document
6) Document -> Analyzer -> storage
7) Analyzer has 3 parts 
		1) Character filters (can have multiple)
		2) Tokenizers (exactly 1)
		3) Token filters
8) Elastic search has builtin Analyzers, character filters, tokenizers, token filters
9) we can also build our own analyzers

Character Filters:
1) Adds, removes, Changes characters
2) Analyzers contains zero or more character fields
3) Character filters are applied in which they are specified
Example: html_strip filter
Input: Im <em>good</em> at it&nsbp!
Output: Im good at it!

Tokenizers:
1) An analyzer contains 1 tokenizer
2) Tokenizes a string means splits into tokens
3) Characters may be stried as part of tokenization
Example:
Input: Im good at it !
Ouptut: ["Im", "good", "at", "it"]

Token Filters:
1) Receives output of Tokenizers as input
2) can have zero or more number of token filters in analyzer
3) Token filters are applied in which they are specified
Example: (Lower case filter)
Input: ["Im", "good", "at", "it"]
Output: ["im", "good", "at", "it"]

Inverted Indexes:
0) values for text fields are analyzed and results are stored in inverted index
1) Mapping between terms and which documents contains them
2) Outside the context of analyzers, we use "TERM" terminology
3) Terms are sorted alphabetically for performance reasons
4) Inverted indices contains more than just terns and documents ids (Example for relevance scoring)
5) One inverted index per text field
6) Other data types use BKD trees
7) Created and maintained by Apache Lucene, not by elastic
8) Inverted indices enables fast Searched

Ex: Consider index has below documents
[
	{
		"id": 1
		"Description": "Hello my name is xyz",
		"country": "india"
	},
	{
		"id": 1
		"Description": "ppl call me lmn",
		"country": "usa"
	}
}

foreach text field it creates a inverted indexes
for Descrition field
		doc1	doc2
call	-		x
hello	x		-
is		x		-
lmn		-		x
me		-		x
my		x		-
name	x		-
ppl		-		x
xyz		x		-

for country field
		doc1	doc2
india	x		-
usa		-		x

#######################################################
#################### Mappings #########################
#######################################################

1) Defines the structure of the document (Ex: fields and their datatypes)
2) Same as schema in rdbms
3) There are 2 types of Mappings
4) Explicit mapping: we define the mappings ourselfs
5) Dynamic mapping: Elasticsearch generates mappings for us

Keyword datatype:
1) Used for exact matches
2) Typically used for filtering, sorting, aggregations
3) For full-text searches uses the text data type instead (ex: searching the body text of article)
4) datatype objects are stored using dot notations internally
5) Keyword fields are analyzed using keyword analyzer
6) keyword field is a no-op analyzer, it outputs unmodified string as a single token
7) The same value is placed in inverted index

for keyword field inverted index would look like
							doc1	doc2
"my entire field value"		x		-
"my doc2 field valuw"		-		x


Example: Objects are stored internally as
{
	"address": {
		"number": "123A",
		"Street": "abc colony"
	}
}

this is stored internally as 

{
	"address.number": "123A",
	"address.Street": "abc Colony"
}



Coercion:
1) Datatypes are inspected when indexing documents
2) data is validated and rejected it not matching
3) example trying to insert object for text datatype
4) inspects mapping fields datatype and coercise into that type if possible
example:
Mappings:
{
	"properties": {
		"cost": { "type": "float"}
	}
}

"7.4"  => coercion  => 7.4
		(analyzes the datatype from mapping and converts into it if possible)

5) "_source" always stores the value that is passed while indexing
6) "_source" object still contains "7.4" because search is performed on the inverted indexex, BKD tress
7) supplying a float value to integer type will truncate the decimal and stores the number
8) coercion is not used for dynamic mapping
9) Always try to use correct values, especially firsttime you index a field
10) coercion is enabled by default, it can be disable according to preference


Introduction to Arrays:
1) In elasticsearch there is not such thing as array datatype
2) if you havent provided mapping it stored as text datatype
Example:
POST /_analyze
{
  "text": ["my name is", "unknown to me"],
  "analyzer": "standard"
}

3) Any field may contain zero or more values
	- No configuration or mapping is needed
	- simply supply a array of values while indexing a document
4) Array values should be of same datatype is the only constraint

Examples: Of correct datatypes
["sita", "geeta"]
[12.9, 45.6]
[true, false, true]
[{"name": "abc"}, {"name": "xyz"}] //Allowed

5) Coercion only works for fields if mappings are already provided
Coercion:
["sita", "geeta"]
[12.9, "45.6"]
[true, false, "true"]

[{"name": "abc"}, {"name": "xyz"}, false] // this is not allowed as it wont be able to coercion

6) In general its not recommended to depend on coercion
7) Array may contain nested arrays
8) Array are flattened during indexing
9) [1, [2,3]] becomes [1,2,3]
10) Remember to use "nested" data type for array of objects if you need to query the objects independently
Suppose you are adding a below document
{
	"name": "nikhil",
	"eduction": [
		{
			"name": "jain university",
			"location": "bangalore"
		},
		{
			"name": "raos college",
			"location": "ndl"
		}
	]
}
for this it inserts 3 documents if you specify education data type as nested

Date Fields:
1) Specified in 3 ways
	- Specially formatted strings
	- Milli seconds since the epoch
	- Seconds since the epoch
2) Epoch reffers to 1st Jan, 1970.
3) Default behaviour of date fields
	- A date without time
	- A date with time
	- long value of millisecs since epoch
4) UTC timezone aussumed if none is specified
5) Dates must for formatted according to ISO 8601 specification
6) Internally dates are stored as epoch as long numbers
7) If a date contains a timezone it is converted to UTC
8) Same happens in case of search queries too
9) Dont provide UNIX timestamps for default fields


Some of the date field values are:
2022-02-23T03:34:55Z //Z means UTC timezone
2022-02-23
2022-02-24T04:45:23+01:30
1424829845923


MISSING FIELDS:
1) All fields in elastic are Optional
2) You can leave out fields while indexing the document
3) Unline relational database where you need to enter null value
4) Adding a field mapping doesnt make field as required field, you can still leave it while creating document
5) So that means you need to handle few things at application level
6) Searches automatically handles missing fields








	


